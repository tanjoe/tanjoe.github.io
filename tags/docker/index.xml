<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docker on Qiao</title>
    <link>https://tanjoe.github.io/tags/docker/</link>
    <description>Recent content in Docker on Qiao</description>
    <image>
      <title>Qiao</title>
      <url>https://tanjoe.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://tanjoe.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 03 Jan 2023 17:54:29 +0000</lastBuildDate><atom:link href="https://tanjoe.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>解决容器内运行conda的GLIBCXX问题</title>
      <link>https://tanjoe.github.io/posts/%E8%A7%A3%E5%86%B3%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cconda%E7%9A%84glibcxx%E9%97%AE%E9%A2%98/</link>
      <pubDate>Tue, 03 Jan 2023 17:54:29 +0000</pubDate>
      
      <guid>https://tanjoe.github.io/posts/%E8%A7%A3%E5%86%B3%E5%AE%B9%E5%99%A8%E5%86%85%E8%BF%90%E8%A1%8Cconda%E7%9A%84glibcxx%E9%97%AE%E9%A2%98/</guid>
      <description>尝试在容器内运行conda，发现报错如下：
conda &amp;#34;ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26&amp;#39; not found&amp;#34; 解决方式 安装libgcc
conda install libgcc 如果仍然报错，则
export LD_LIBRARY_PATH=&amp;lt;conda-env-path&amp;gt;/lib:$LD_LIBRARY_PATH conda-env-path替换为conda的目录，核心是通过修改LD_LIBRARY_PATH，让conda的python正确加载conda安装的libstdc++，而非系统的版本
To-Ask 为什么需要配环境变量？
按理来说conda在安装时应当已经修改了bashrc、zshrc等文件，让shell环境里的LD_LIBRARY_PATH带上了conda的lib目录（需要检查下未挂载home目录的容器以确认）。有可能是因为在启动容器时挂载了整个home目录，导致容器内的bash用了host的配置所致。
仔细想想，虽然挂载整个home目录用起来方便，但像shell配置、各类软件的cache本身是应该与host独立的，最好还是挂载home下的特定目录</description>
    </item>
    
    <item>
      <title>在容器内使用显卡进行渲染</title>
      <link>https://tanjoe.github.io/posts/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E4%BD%BF%E7%94%A8%E6%98%BE%E5%8D%A1%E8%BF%9B%E8%A1%8C%E6%B8%B2%E6%9F%93/</link>
      <pubDate>Sun, 01 Jan 2023 10:01:29 +0000</pubDate>
      
      <guid>https://tanjoe.github.io/posts/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E4%BD%BF%E7%94%A8%E6%98%BE%E5%8D%A1%E8%BF%9B%E8%A1%8C%E6%B8%B2%E6%9F%93/</guid>
      <description>由于要测试PyTorch3D生成的Mesh，而PyTorch3D的环境在本机又不好搭建，准备在Docker容器内做些渲染相关的工作，按关键词nvidia opengl docker搜索了一番，发现应该很好完成。
结合几篇文章的内容，以及nvidia/opengl的Dockerfile来看，核心是安装libglvnd0及一些依赖库、配置好glvnd的vendor json文件、设置nvidia docker的环境变量即可，但一番操作下来，在容器内安装mesa-utils和glmark2后，用glxinfo和glmark2都显示vendor是：
OpenGL renderer string: llvmpipe (LLVM 11.0.0, 256 bits) 即仍然在使用软渲染，但nvidia-smi在容器内工作又是正常的。
使用集显 在Host机器上测试glxinfo和glmark2，发现vendor居然也不是nvidia而是MESA Intel，也就是电脑的集成显卡。不过集成显卡就集成显卡吧，好歹让容器能够用集显，这样工作好歹可以继续。
测试后，发现在docker run时附带--device=/dev/dri:/dev/dri参数即可。这样操作后，容器内glxinfo总算显示vendor是集成显卡了，由于工作要求的渲染性能不高，代码倒也能跑起来。
关于Linux DRI，可参考Linux graphic subsystem(2)_DRI介绍的说明
使用独显 话说回来，为什么host和容器都显示vendor是集显而非Nvidia的独显？明明nvidia-smi工作正常，CUDA的代码也能运行。带此疑问，用why glxinfo not detect nvidia while nividia-smi works搜索一番，发现Nvidia的论坛里也有些相似的问题，但求助都没有明确答复。
最后本机上打开nvidia-settings查看设置时，发现Profile里的3个选项：
Nvidia (Performance mode) Nvidia On-Demand Intel (Power saving mode) 第3个很好理解，但Nvidia Performance mode和On-Demand又有什么区别？搜索一番，发现此贴：
Nvidia On - Demand : Ubuntu
there&amp;rsquo;s a good write up here: https://www.linuxuprising.com/2019/08/nvidia-43517-linux-beta-driver-adds.html
on-demand means the Ubuntu optimus tool now lets you have dynamic switching of nvidia, but output is limited to the laptop screen, which is &amp;lsquo;bumblebee mode&amp;rsquo;, or the normal ubuntu Nvidia mode, which turns on the card after you restart X; this mode uses nvidia to render everything, and external monitors work.</description>
    </item>
    
    <item>
      <title>Use docker with Nvidia GPU in WSL2</title>
      <link>https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/</link>
      <pubDate>Tue, 13 Dec 2022 20:13:00 +0000</pubDate>
      
      <guid>https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/</guid>
      <description>GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行
docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于
Run &amp;#34;nbody -benchmark [-numbodies=&amp;lt;numBodies&amp;gt;]&amp;#34; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=&amp;lt;N&amp;gt; (number of bodies (&amp;gt;= 1) to run in simulation) -device=&amp;lt;d&amp;gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=&amp;lt;i&amp;gt; (where i=(number of CUDA devices &amp;gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=&amp;lt;file.</description>
    </item>
    
  </channel>
</rss>
