<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Use docker with Nvidia GPU in WSL2 | Qiao</title><meta name=keywords content="Docker,CUDA,WSL2"><meta name=description content="GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行
docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于
Run &#34;nbody -benchmark [-numbodies=<numBodies>]&#34; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file."><meta name=author content="Qiao"><link rel=canonical href=https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>const config={startOnLoad:!0,theme:"forest",themeVariables:{lineColor:"#fafafa"},flowchart:{useMaxWidth:!1,htmlLabels:!0}};mermaid.initialize(config),window.onload=()=>{window.mermaid.init(void 0,document.querySelectorAll(".language-mermaid"))}</script><meta property="og:title" content="Use docker with Nvidia GPU in WSL2"><meta property="og:description" content="GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行
docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于
Run &#34;nbody -benchmark [-numbodies=<numBodies>]&#34; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file."><meta property="og:type" content="article"><meta property="og:url" content="https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/"><meta property="og:image" content="https://tanjoe.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-13T20:13:00+00:00"><meta property="article:modified_time" content="2022-12-13T20:13:00+00:00"><meta property="og:site_name" content="Qiao"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tanjoe.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Use docker with Nvidia GPU in WSL2"><meta name=twitter:description content="GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行
docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于
Run &#34;nbody -benchmark [-numbodies=<numBodies>]&#34; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=<N> (number of bodies (>= 1) to run in simulation) -device=<d> (where d=0,1,2.... for the CUDA device to use) -numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=<file."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tanjoe.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Use docker with Nvidia GPU in WSL2","item":"https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Use docker with Nvidia GPU in WSL2","name":"Use docker with Nvidia GPU in WSL2","description":"GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行\ndocker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于\nRun \u0026#34;nbody -benchmark [-numbodies=\u0026lt;numBodies\u0026gt;]\u0026#34; to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=\u0026lt;N\u0026gt; (number of bodies (\u0026gt;= 1) to run in simulation) -device=\u0026lt;d\u0026gt; (where d=0,1,2.... for the CUDA device to use) -numdevices=\u0026lt;i\u0026gt; (where i=(number of CUDA devices \u0026gt; 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=\u0026lt;file.","keywords":["Docker","CUDA","WSL2"],"articleBody":"GPU Support 先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行wsl --update更新wsl。完成后，终端执行\ndocker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark 如果GPU可用，则输出类似于\nRun \"nbody -benchmark [-numbodies=]\" to measure performance. -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies=\u003cN\u003e (number of bodies (\u003e= 1) to run in simulation) -device=\u003cd\u003e (where d=0,1,2.... for the CUDA device to use) -numdevices=\u003ci\u003e (where i=(number of CUDA devices \u003e 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run n-body simulation on the CPU) -tipsy=\u003cfile.bin\u003e (load a tipsy model file for simulation) \u003e NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. \u003e Windowed mode \u003e Simulation data stored in video memory \u003e Single precision floating point simulation \u003e 1 Devices used for simulation MapSMtoCores for SM 7.5 is undefined. Default to use 64 Cores/SM GPU Device 0: \"GeForce RTX 2060 with Max-Q Design\" with compute capability 7.5 \u003e Compute 7.5 CUDA device: [GeForce RTX 2060 with Max-Q Design] 30720 bodies, total time for 10 iterations: 69.280 ms = 136.219 billion interactions per second = 2724.379 single-precision GFLOP/s at 20 flops per interaction Use docker in WSL Docker Desktop的settings-resources-WSL Integration勾选\"Enable intergration with my default WSL distro\"以及所需的发行版，点击\"Refresh\"，随用Windows Terminal新打开WSL发行版的终端即可。输入\ndocker --version \u003e Docker version 20.10.21, build baeda1f 有版本号说明docker集成正常\nUse docker with GPU in WSL 完成以上步骤后，在WSL里同样可以用docker run --gpus=all的方式在WSL的docker里启用GPU，例如：\n# 注意此命令是在WSL而非Windows里执行的 docker run -it --name \"nimble-build\" --restart always --user \"${USER}\" --workdir \"${PWD}\" --env LANG=zh_CN.UTF-8 --env TERM=xterm-256color --gpus=all --volume=\"$HOME\":\"$HOME\" --detach \"nimble-image\" /bin/bash 测试nvidia-smi在docker里是否工作正常：\ndocker exec -it \"nimble-build\" nvidia-smi Tue Dec 13 11:58:05 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 516.94 CUDA Version: 11.7 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:01:00.0 On | N/A | | 0% 37C P8 9W / 130W | 932MiB / 8192MiB | 15% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 容器实际上运行在Windows的Docker上（而非WSL系统），因而在Windows终端可以查看到该容器：\n# windows powershell内执行 docker container list CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 597d09d2c09c nimble-image \"/bin/bash\" 9 hours ago Up 3 hours nimble-build 并且也可以用VS Code远程到此容器进行开发。\n题外话：修改WSL发行版的密码 WSL发行版的root是没有设置密码的，因而如果忘了当前用户的密码，可以在Windows终端内执行\nubuntu2004.exe config --default-user root # 视WSL发行版的不同，有可能是ubuntu.exe或其它可执行文件，ubuntu2004对应WSL Ubuntu 20.04 切换WSL发行版登录用户为root。以root登录后，用\npasswd ${用户} 修改对应用户的密码即可。完成后再用\nubuntu2004.exe config --default-user ${用户} 切换默认用户回去即可。\n参考 Docker Desktop WSL 2 backend on Windows | Docker Documentation\nHow to Reset Ubuntu Linux Password on WSL [In 3 Easy Steps]\n","wordCount":"448","inLanguage":"en","datePublished":"2022-12-13T20:13:00Z","dateModified":"2022-12-13T20:13:00Z","author":{"@type":"Person","name":"Qiao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tanjoe.github.io/posts/use-docker-with-nvidia-gpu-in-wsl2/"},"publisher":{"@type":"Organization","name":"Qiao","logo":{"@type":"ImageObject","url":"https://tanjoe.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tanjoe.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://tanjoe.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tanjoe.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://tanjoe.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://tanjoe.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://tanjoe.github.io/posts/>Posts</a></div><h1 class=post-title>Use docker with Nvidia GPU in WSL2</h1><div class=post-meta><span title='2022-12-13 20:13:00 +0000 UTC'>2022-12-13</span>&nbsp;·&nbsp;Qiao</div></header><div class=post-content><h2 id=gpu-support>GPU Support<a hidden class=anchor aria-hidden=true href=#gpu-support>#</a></h2><p>先确认Docker Desktop的Backend使用的是WSL2，并且Windows、Nvidia驱动的版本足够，随后管理员权限终端执行<code>wsl --update</code>更新wsl。完成后，终端执行</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=n>docker</span> <span class=n>run</span> <span class=p>-</span><span class=n>-rm</span> <span class=n>-it</span> <span class=p>-</span><span class=n>-gpus</span><span class=p>=</span><span class=n>all</span> <span class=n>nvcr</span><span class=p>.</span><span class=n>io</span><span class=p>/</span><span class=n>nvidia</span><span class=p>/</span><span class=n>k8s</span><span class=p>/</span><span class=nb>cuda-sample</span><span class=err>:</span><span class=n>nbody</span> <span class=n>nbody</span> <span class=n>-gpu</span> <span class=n>-benchmark</span>
</span></span></code></pre></div><p>如果GPU可用，则输出类似于</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=n>Run</span> <span class=s2>&#34;nbody -benchmark [-numbodies=&lt;numBodies&gt;]&#34;</span> <span class=n>to</span> <span class=nb>measure </span><span class=n>performance</span><span class=p>.</span>
</span></span><span class=line><span class=cl>        <span class=n>-fullscreen</span>       <span class=p>(</span><span class=n>run</span> <span class=nb>n-body</span> <span class=n>simulation</span> <span class=k>in</span> <span class=n>fullscreen</span> <span class=n>mode</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-fp64</span>             <span class=p>(</span><span class=n>use</span> <span class=n>double</span> <span class=n>precision</span> <span class=n>floating</span> <span class=n>point</span> <span class=n>values</span> <span class=k>for</span> <span class=n>simulation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-hostmem</span>          <span class=p>(</span><span class=n>stores</span> <span class=n>simulation</span> <span class=n>data</span> <span class=k>in</span> <span class=n>host</span> <span class=n>memory</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-benchmark</span>        <span class=p>(</span><span class=n>run</span> <span class=n>benchmark</span> <span class=n>to</span> <span class=nb>measure </span><span class=n>performance</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-numbodies</span><span class=p>=&lt;</span><span class=n>N</span><span class=p>&gt;</span>    <span class=p>(</span><span class=n>number</span> <span class=n>of</span> <span class=n>bodies</span> <span class=p>(&gt;=</span> <span class=mf>1</span><span class=p>)</span> <span class=n>to</span> <span class=n>run</span> <span class=k>in</span> <span class=n>simulation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-device</span><span class=p>=&lt;</span><span class=n>d</span><span class=p>&gt;</span>       <span class=p>(</span><span class=nb>where </span><span class=n>d</span><span class=p>=</span><span class=mf>0</span><span class=p>,</span><span class=mf>1</span><span class=p>,</span><span class=mf>2</span><span class=p>....</span> <span class=k>for</span> <span class=n>the</span> <span class=n>CUDA</span> <span class=n>device</span> <span class=n>to</span> <span class=n>use</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-numdevices</span><span class=p>=&lt;</span><span class=n>i</span><span class=p>&gt;</span>   <span class=p>(</span><span class=nb>where </span><span class=n>i</span><span class=p>=(</span><span class=n>number</span> <span class=n>of</span> <span class=n>CUDA</span> <span class=n>devices</span> <span class=p>&gt;</span> <span class=mf>0</span><span class=p>)</span> <span class=n>to</span> <span class=n>use</span> <span class=k>for</span> <span class=n>simulation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-compare</span>          <span class=p>(</span><span class=n>compares</span> <span class=n>simulation</span> <span class=n>results</span> <span class=n>running</span> <span class=n>once</span> <span class=n>on</span> <span class=n>the</span> <span class=k>default</span> <span class=n>GPU</span> <span class=n>and</span> <span class=n>once</span> <span class=n>on</span> <span class=n>the</span> <span class=n>CPU</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-cpu</span>              <span class=p>(</span><span class=n>run</span> <span class=nb>n-body</span> <span class=n>simulation</span> <span class=n>on</span> <span class=n>the</span> <span class=n>CPU</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>-tipsy</span><span class=p>=&lt;</span><span class=n>file</span><span class=p>.</span><span class=n>bin</span><span class=p>&gt;</span> <span class=p>(</span><span class=n>load</span> <span class=n>a</span> <span class=n>tipsy</span> <span class=n>model</span> <span class=n>file</span> <span class=k>for</span> <span class=n>simulation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=n>NOTE</span><span class=err>:</span> <span class=n>The</span> <span class=n>CUDA</span> <span class=n>Samples</span> <span class=n>are</span> <span class=n>not</span> <span class=n>meant</span> <span class=k>for</span> <span class=n>performance</span> <span class=n>measurements</span><span class=p>.</span> <span class=n>Results</span> <span class=n>may</span> <span class=n>vary</span> <span class=n>when</span> <span class=n>GPU</span> <span class=n>Boost</span> <span class=n>is</span> <span class=n>enabled</span><span class=p>.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=n>Windowed</span> <span class=n>mode</span>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=n>Simulation</span> <span class=n>data</span> <span class=n>stored</span> <span class=k>in</span> <span class=n>video</span> <span class=n>memory</span>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=n>Single</span> <span class=n>precision</span> <span class=n>floating</span> <span class=n>point</span> <span class=n>simulation</span>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=mf>1</span> <span class=n>Devices</span> <span class=n>used</span> <span class=k>for</span> <span class=n>simulation</span>
</span></span><span class=line><span class=cl><span class=n>MapSMtoCores</span> <span class=k>for</span> <span class=n>SM</span> <span class=mf>7.5</span> <span class=n>is</span> <span class=n>undefined</span><span class=p>.</span>  <span class=k>Default</span> <span class=n>to</span> <span class=n>use</span> <span class=mf>64</span> <span class=n>Cores</span><span class=p>/</span><span class=n>SM</span>
</span></span><span class=line><span class=cl><span class=n>GPU</span> <span class=n>Device</span> <span class=mf>0</span><span class=err>:</span> <span class=s2>&#34;GeForce RTX 2060 with Max-Q Design&#34;</span> <span class=n>with</span> <span class=n>compute</span> <span class=n>capability</span> <span class=mf>7.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>&gt;</span> <span class=n>Compute</span> <span class=mf>7.5</span> <span class=n>CUDA</span> <span class=n>device</span><span class=err>:</span> <span class=p>[</span><span class=n>GeForce</span> <span class=n>RTX</span> <span class=mf>2060</span> <span class=n>with</span> <span class=nb>Max-Q</span> <span class=n>Design</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=mf>30720</span> <span class=n>bodies</span><span class=p>,</span> <span class=n>total</span> <span class=n>time</span> <span class=k>for</span> <span class=mf>10</span> <span class=n>iterations</span><span class=err>:</span> <span class=mf>69.280</span> <span class=n>ms</span>
</span></span><span class=line><span class=cl><span class=p>=</span> <span class=mf>136.219</span> <span class=n>billion</span> <span class=n>interactions</span> <span class=n>per</span> <span class=n>second</span>
</span></span><span class=line><span class=cl><span class=p>=</span> <span class=mf>2724.379</span> <span class=nb>single-precision</span> <span class=n>GFLOP</span><span class=p>/</span><span class=n>s</span> <span class=n>at</span> <span class=mf>20</span> <span class=n>flops</span> <span class=n>per</span> <span class=n>interaction</span>
</span></span></code></pre></div><h2 id=use-docker-in-wsl>Use docker in WSL<a hidden class=anchor aria-hidden=true href=#use-docker-in-wsl>#</a></h2><p>Docker Desktop的<code>settings-resources-WSL Integration</code>勾选"Enable intergration with my default WSL distro"以及所需的发行版，点击"Refresh"，随用Windows Terminal新打开WSL发行版的终端即可。输入</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker --version
</span></span><span class=line><span class=cl>&gt; Docker version 20.10.21, build baeda1f
</span></span></code></pre></div><p>有版本号说明docker集成正常</p><h2 id=use-docker-with-gpu-in-wsl>Use docker with GPU in WSL<a hidden class=anchor aria-hidden=true href=#use-docker-with-gpu-in-wsl>#</a></h2><p>完成以上步骤后，在WSL里同样可以用<code>docker run --gpus=all</code>的方式在WSL的docker里启用GPU，例如：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 注意此命令是在WSL而非Windows里执行的</span>
</span></span><span class=line><span class=cl>docker run -it --name <span class=s2>&#34;nimble-build&#34;</span> --restart always --user <span class=s2>&#34;</span><span class=si>${</span><span class=nv>USER</span><span class=si>}</span><span class=s2>&#34;</span> --workdir <span class=s2>&#34;</span><span class=si>${</span><span class=nv>PWD</span><span class=si>}</span><span class=s2>&#34;</span> --env <span class=nv>LANG</span><span class=o>=</span>zh_CN.UTF-8 --env <span class=nv>TERM</span><span class=o>=</span>xterm-256color --gpus<span class=o>=</span>all --volume<span class=o>=</span><span class=s2>&#34;</span><span class=nv>$HOME</span><span class=s2>&#34;</span>:<span class=s2>&#34;</span><span class=nv>$HOME</span><span class=s2>&#34;</span> --detach <span class=s2>&#34;nimble-image&#34;</span> /bin/bash
</span></span></code></pre></div><p>测试<code>nvidia-smi</code>在docker里是否工作正常：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker <span class=nb>exec</span> -it <span class=s2>&#34;nimble-build&#34;</span> nvidia-smi
</span></span><span class=line><span class=cl>Tue Dec <span class=m>13</span> 11:58:05 <span class=m>2022</span>
</span></span><span class=line><span class=cl>+-----------------------------------------------------------------------------+
</span></span><span class=line><span class=cl><span class=p>|</span> NVIDIA-SMI 515.65.01    Driver Version: 516.94       CUDA Version: 11.7     <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>-------------------------------+----------------------+----------------------+
</span></span><span class=line><span class=cl><span class=p>|</span> GPU  Name        Persistence-M<span class=p>|</span> Bus-Id        Disp.A <span class=p>|</span> Volatile Uncorr. ECC <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class=p>|</span>         Memory-Usage <span class=p>|</span> GPU-Util  Compute M. <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>                               <span class=p>|</span>                      <span class=p>|</span>               MIG M. <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span><span class=o>===============================</span>+<span class=o>======================</span>+<span class=o>======================</span><span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   <span class=m>0</span>  NVIDIA GeForce ...  On   <span class=p>|</span> 00000000:01:00.0  On <span class=p>|</span>                  N/A <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  0%   37C    P8     9W / 130W <span class=p>|</span>    932MiB /  8192MiB <span class=p>|</span>     15%      Default <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>                               <span class=p>|</span>                      <span class=p>|</span>                  N/A <span class=p>|</span>
</span></span><span class=line><span class=cl>+-------------------------------+----------------------+----------------------+
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>+-----------------------------------------------------------------------------+
</span></span><span class=line><span class=cl><span class=p>|</span> Processes:                                                                  <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  GPU   GI   CI        PID   Type   Process name                  GPU Memory <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>        ID   ID                                                   Usage      <span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span><span class=o>=============================================================================</span><span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  No running processes found                                                 <span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------------------------------------------------------------------+
</span></span></code></pre></div><p>容器实际上运行在Windows的Docker上（而非WSL系统），因而在Windows终端可以查看到该容器：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=c># windows powershell内执行</span>
</span></span><span class=line><span class=cl><span class=n>docker</span> <span class=n>container</span> <span class=n>list</span>
</span></span><span class=line><span class=cl><span class=n>CONTAINER</span> <span class=n>ID</span>   <span class=n>IMAGE</span>          <span class=n>COMMAND</span>       <span class=n>CREATED</span>       <span class=n>STATUS</span>       <span class=n>PORTS</span>     <span class=n>NAMES</span>
</span></span><span class=line><span class=cl><span class=n>597d09d2c09c</span>   <span class=nb>nimble-image</span>   <span class=s2>&#34;/bin/bash&#34;</span>   <span class=mf>9</span> <span class=n>hours</span> <span class=n>ago</span>   <span class=n>Up</span> <span class=mf>3</span> <span class=n>hours</span>             <span class=nb>nimble-build</span>
</span></span></code></pre></div><p>并且也可以用VS Code远程到此容器进行开发。</p><h2 id=题外话修改wsl发行版的密码>题外话：修改WSL发行版的密码<a hidden class=anchor aria-hidden=true href=#题外话修改wsl发行版的密码>#</a></h2><p>WSL发行版的root是没有设置密码的，因而如果忘了当前用户的密码，可以在Windows终端内执行</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=n>ubuntu2004</span><span class=p>.</span><span class=py>exe</span> <span class=n>config</span> <span class=p>-</span><span class=n>-default-user</span> <span class=n>root</span> <span class=c># 视WSL发行版的不同，有可能是ubuntu.exe或其它可执行文件，ubuntu2004对应WSL Ubuntu 20.04</span>
</span></span></code></pre></div><p>切换WSL发行版登录用户为root。以root登录后，用</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>passwd <span class=si>${</span><span class=p>用户</span><span class=si>}</span>
</span></span></code></pre></div><p>修改对应用户的密码即可。完成后再用</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl> <span class=n>ubuntu2004</span><span class=p>.</span><span class=py>exe</span> <span class=n>config</span> <span class=p>-</span><span class=n>-default-user</span> <span class=p>${</span><span class=err>用户</span><span class=p>}</span>
</span></span></code></pre></div><p>切换默认用户回去即可。</p><h2 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h2><p><a href=https://docs.docker.com/desktop/windows/wsl/>Docker Desktop WSL 2 backend on Windows | Docker Documentation</a></p><p><a href=https://itsfoss.com/reset-linux-password-wsl/>How to Reset Ubuntu Linux Password on WSL [In 3 Easy Steps]</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tanjoe.github.io/tags/docker/>Docker</a></li><li><a href=https://tanjoe.github.io/tags/cuda/>CUDA</a></li><li><a href=https://tanjoe.github.io/tags/wsl2/>WSL2</a></li></ul><nav class=paginav><a class=prev href=https://tanjoe.github.io/posts/%E5%9C%A8%E5%AE%B9%E5%99%A8%E5%86%85%E4%BD%BF%E7%94%A8%E6%98%BE%E5%8D%A1%E8%BF%9B%E8%A1%8C%E6%B8%B2%E6%9F%93/><span class=title>« Prev</span><br><span>在容器内使用显卡进行渲染</span></a>
<a class=next href=https://tanjoe.github.io/posts/%E4%BD%BF%E7%94%A8github-actions%E9%83%A8%E7%BD%B2hexo/><span class=title>Next »</span><br><span>使用Github Actions部署Hexo</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://tanjoe.github.io/>Qiao</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>